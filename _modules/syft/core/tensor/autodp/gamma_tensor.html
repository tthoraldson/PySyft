
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>syft.core.tensor.autodp.gamma_tensor &#8212; PySyft  documentation</title>
    
  <link href="../../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/pysyft.css" />
    
  <link rel="preload" as="script" href="../../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../_static/jquery.js"></script>
    <script src="../../../../../_static/underscore.js"></script>
    <script src="../../../../../_static/doctools.js"></script>
    <script src="../../../../../_static/clipboard.min.js"></script>
    <script src="../../../../../_static/copybutton.js"></script>
    <link rel="shortcut icon" href="../../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../../index.html">
  <img src="../../../../../_static/logo.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../../api_reference/index.html">
  API reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../../developer_guide/index.html">
  Contributor Guidelines
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../../deployment/glossary.html">
  Glossary
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../../resources/index.html">
  Resources
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../../guides/index.html">
  How-to Guides
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/OpenMined/PySyft" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/openminedorg" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for syft.core.tensor.autodp.gamma_tensor</h1><div class="highlight"><pre>
<span></span><span class="c1"># future</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="c1"># stdlib</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Deque</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Sequence</span>

<span class="c1"># from dataclasses import replace</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="c1"># third party</span>
<span class="kn">import</span> <span class="nn">flax</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># from numpy.random import randint</span>
<span class="kn">from</span> <span class="nn">numpy.typing</span> <span class="kn">import</span> <span class="n">ArrayLike</span>

<span class="c1"># from scipy import optimize</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">shgo</span>

<span class="c1"># relative</span>
<span class="kn">from</span> <span class="nn">....</span> <span class="kn">import</span> <span class="n">lib</span>
<span class="kn">from</span> <span class="nn">....ast.klass</span> <span class="kn">import</span> <span class="n">pointerize_args_and_kwargs</span>

<span class="c1"># from ....core.adp.data_subject import DataSubject</span>
<span class="kn">from</span> <span class="nn">....core.node.common.action.get_or_set_property_action</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">GetOrSetPropertyAction</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">....core.node.common.action.get_or_set_property_action</span> <span class="kn">import</span> <span class="n">PropertyActions</span>
<span class="kn">from</span> <span class="nn">....lib.numpy.array</span> <span class="kn">import</span> <span class="n">capnp_deserialize</span>
<span class="kn">from</span> <span class="nn">....lib.numpy.array</span> <span class="kn">import</span> <span class="n">capnp_serialize</span>
<span class="kn">from</span> <span class="nn">....lib.python.util</span> <span class="kn">import</span> <span class="n">upcast</span>
<span class="kn">from</span> <span class="nn">....util</span> <span class="kn">import</span> <span class="n">inherit_tags</span>
<span class="kn">from</span> <span class="nn">...adp.data_subject_ledger</span> <span class="kn">import</span> <span class="n">DataSubjectLedger</span>

<span class="c1"># from ...adp.data_subject_list import DataSubjectList</span>
<span class="c1"># from ...adp.data_subject_list import DataSubjectArray</span>
<span class="c1"># from ...adp.data_subject_list import dslarraytonumpyutf8</span>
<span class="c1"># from ...adp.data_subject_list import numpyutf8todslarray</span>
<span class="kn">from</span> <span class="nn">...adp.vectorized_publish</span> <span class="kn">import</span> <span class="n">publish</span>
<span class="kn">from</span> <span class="nn">...common.serde.capnp</span> <span class="kn">import</span> <span class="n">CapnpModule</span>
<span class="kn">from</span> <span class="nn">...common.serde.capnp</span> <span class="kn">import</span> <span class="n">chunk_bytes</span>
<span class="kn">from</span> <span class="nn">...common.serde.capnp</span> <span class="kn">import</span> <span class="n">combine_bytes</span>
<span class="kn">from</span> <span class="nn">...common.serde.capnp</span> <span class="kn">import</span> <span class="n">get_capnp_schema</span>
<span class="kn">from</span> <span class="nn">...common.serde.capnp</span> <span class="kn">import</span> <span class="n">serde_magic_header</span>
<span class="kn">from</span> <span class="nn">...common.serde.deserialize</span> <span class="kn">import</span> <span class="n">_deserialize</span> <span class="k">as</span> <span class="n">deserialize</span>
<span class="kn">from</span> <span class="nn">...common.serde.serializable</span> <span class="kn">import</span> <span class="n">serializable</span>
<span class="kn">from</span> <span class="nn">...common.serde.serialize</span> <span class="kn">import</span> <span class="n">_serialize</span> <span class="k">as</span> <span class="n">serialize</span>
<span class="kn">from</span> <span class="nn">...common.uid</span> <span class="kn">import</span> <span class="n">UID</span>
<span class="kn">from</span> <span class="nn">...node.abstract.node</span> <span class="kn">import</span> <span class="n">AbstractNodeClient</span>
<span class="kn">from</span> <span class="nn">...node.common.action.run_class_method_action</span> <span class="kn">import</span> <span class="n">RunClassMethodAction</span>
<span class="kn">from</span> <span class="nn">...node.enums</span> <span class="kn">import</span> <span class="n">PointerStatus</span>
<span class="kn">from</span> <span class="nn">...pointer.pointer</span> <span class="kn">import</span> <span class="n">Pointer</span>
<span class="kn">from</span> <span class="nn">..config</span> <span class="kn">import</span> <span class="n">DEFAULT_INT_NUMPY_TYPE</span>
<span class="kn">from</span> <span class="nn">..fixed_precision_tensor</span> <span class="kn">import</span> <span class="n">FixedPrecisionTensor</span>
<span class="kn">from</span> <span class="nn">..passthrough</span> <span class="kn">import</span> <span class="n">PassthroughTensor</span>  <span class="c1"># type: ignore</span>
<span class="kn">from</span> <span class="nn">..smpc</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">..smpc.mpc_tensor</span> <span class="kn">import</span> <span class="n">MPCTensor</span>
<span class="kn">from</span> <span class="nn">..smpc.utils</span> <span class="kn">import</span> <span class="n">TYPE_TO_RING_SIZE</span>
<span class="kn">from</span> <span class="nn">..util</span> <span class="kn">import</span> <span class="n">implements</span>
<span class="kn">from</span> <span class="nn">.gamma_tensor_ops</span> <span class="kn">import</span> <span class="n">GAMMA_TENSOR_OP</span>
<span class="kn">from</span> <span class="nn">.gamma_tensor_ops</span> <span class="kn">import</span> <span class="n">GAMMA_TENSOR_OP_FUNC</span>
<span class="kn">from</span> <span class="nn">.jax_ops</span> <span class="kn">import</span> <span class="n">SyftJaxInfixOp</span>
<span class="kn">from</span> <span class="nn">.jax_ops</span> <span class="kn">import</span> <span class="n">SyftJaxOp</span>
<span class="kn">from</span> <span class="nn">.jax_ops</span> <span class="kn">import</span> <span class="n">SyftJaxUnaryOp</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># stdlib</span>
    <span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># third party</span>
    <span class="kn">from</span> <span class="nn">flax.struct</span> <span class="kn">import</span> <span class="n">dataclass</span>


<span class="n">INPLACE_OPS</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;resize&quot;</span><span class="p">,</span> <span class="s2">&quot;sort&quot;</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">debox_other</span><span class="p">(</span><span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">attr</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">GammaTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">other</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">debox_child</span><span class="p">(</span><span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">debox_other</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="s2">&quot;child&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">debox_linear</span><span class="p">(</span><span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">GammaTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">debox_other</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="s2">&quot;is_linear&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">debox_phi</span><span class="p">(</span><span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="c1"># relative</span>
    <span class="kn">from</span> <span class="nn">.phi_tensor</span> <span class="kn">import</span> <span class="n">PhiTensor</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">PhiTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">other</span>
    <span class="k">return</span> <span class="n">other</span><span class="o">.</span><span class="n">gamma</span>


<span class="k">def</span> <span class="nf">update_state</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">GammaTensor</span><span class="p">):</span>
        <span class="n">state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">sources</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span>


<span class="n">SingleOrTupleInt</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
<span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SingleOrTupleInt</span><span class="p">]</span>


<div class="viewcode-block" id="TensorWrappedGammaTensorPointer"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer">[docs]</a><span class="nd">@serializable</span><span class="p">(</span><span class="n">recursive_serde</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorWrappedGammaTensorPointer</span><span class="p">(</span><span class="n">Pointer</span><span class="p">):</span>
    <span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;TensorWrappedGammaTensorPointer&quot;</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;syft.core.tensor.autodp.gamma_tensor&quot;</span>
    <span class="n">__attr_allowlist__</span> <span class="o">=</span> <span class="p">[</span>
        <span class="c1"># default pointer attrs</span>
        <span class="s2">&quot;client&quot;</span><span class="p">,</span>
        <span class="s2">&quot;id_at_location&quot;</span><span class="p">,</span>
        <span class="s2">&quot;object_type&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tags&quot;</span><span class="p">,</span>
        <span class="s2">&quot;description&quot;</span><span class="p">,</span>
        <span class="c1"># phi_tensor attrs</span>
        <span class="s2">&quot;public_dtype&quot;</span><span class="p">,</span>
        <span class="s2">&quot;public_shape&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">__serde_overrides__</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;client&quot;</span><span class="p">:</span> <span class="p">[</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">address</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">y</span><span class="p">],</span>
        <span class="s2">&quot;public_shape&quot;</span><span class="p">:</span> <span class="p">[</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">upcast</span><span class="p">(</span><span class="n">y</span><span class="p">)],</span>
        <span class="c1"># &quot;data_subjects&quot;: [dslarraytonumpyutf8, numpyutf8todslarray],</span>
        <span class="s2">&quot;public_dtype&quot;</span><span class="p">:</span> <span class="p">[</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">y</span><span class="p">)],</span>
    <span class="p">}</span>
    <span class="n">_exhausted</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">is_enum</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">PUBLISH_POINTER_TYPE</span> <span class="o">=</span> <span class="s2">&quot;numpy.ndarray&quot;</span>
    <span class="n">__array_ufunc__</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">client</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">id_at_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UID</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">object_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">tags</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">public_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">public_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
            <span class="n">id_at_location</span><span class="o">=</span><span class="n">id_at_location</span><span class="p">,</span>
            <span class="n">object_type</span><span class="o">=</span><span class="n">object_type</span><span class="p">,</span>
            <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="n">description</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span> <span class="o">=</span> <span class="n">public_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span> <span class="o">=</span> <span class="n">public_dtype</span>

    <span class="c1"># TODO: Modify for large arrays</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">synthetic</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">public_dtype_func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span><span class="p">,</span> <span class="s2">&quot;upcast&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
            <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_vals</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_vals</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_vals</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
        <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">public_dtype_func</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">repr_string</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;PointerId: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">id_at_location</span><span class="o">.</span><span class="n">no_dash</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span> <span class="s2">&quot;obj_exists&quot;</span><span class="p">):</span>
            <span class="n">_ptr_status</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">PointerStatus</span><span class="o">.</span><span class="n">READY</span><span class="o">.</span><span class="n">value</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exists</span>
                <span class="k">else</span> <span class="n">PointerStatus</span><span class="o">.</span><span class="n">PROCESSING</span><span class="o">.</span><span class="n">value</span>
            <span class="p">)</span>
            <span class="n">repr_string</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Status: </span><span class="si">{</span><span class="n">_ptr_status</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">repr_string</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Representation: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">synthetic</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">repr_string</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">(The data printed above is synthetic - it&#39;s an imitation of the real data.)&quot;</span>
        <span class="k">return</span> <span class="n">repr_string</span>

    <span class="k">def</span> <span class="nf">share</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">parties</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">AbstractNodeClient</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">MPCTensor</span><span class="p">:</span>
        <span class="n">all_parties</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">parties</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">]</span>
        <span class="n">ring_size</span> <span class="o">=</span> <span class="n">TYPE_TO_RING_SIZE</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">self_mpc</span> <span class="o">=</span> <span class="n">MPCTensor</span><span class="p">(</span>
            <span class="n">secret</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span>
            <span class="n">ring_size</span><span class="o">=</span><span class="n">ring_size</span><span class="p">,</span>
            <span class="n">parties</span><span class="o">=</span><span class="n">all_parties</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">self_mpc</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;public_shape&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_apply_tensor_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">op_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># we want to get the return type which matches the attr_path_and_name</span>
        <span class="c1"># so we ask lib_ast for the return type name that matches out</span>
        <span class="c1"># attr_path_and_name and then use that to get the actual pointer klass</span>
        <span class="c1"># then set the result to that pointer klass</span>
        <span class="c1"># We always maintain a Tensor hierarchy Tensor ---&gt; PT--&gt; Actual Data</span>
        <span class="n">attr_path_and_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;syft.core.tensor.tensor.Tensor.</span><span class="si">{</span><span class="n">op_str</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">(</span>
            <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># QUESTION can the id_at_location be None?</span>
        <span class="n">result_id_at_location</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s2">&quot;id_at_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result_id_at_location</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># first downcast anything primitive which is not already PyPrimitive</span>
            <span class="p">(</span>
                <span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">downcast_kwargs</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">downcast_args_and_kwargs</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">other</span><span class="p">],</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{})</span>

            <span class="c1"># then we convert anything which isnt a pointer into a pointer</span>
            <span class="n">pointer_args</span><span class="p">,</span> <span class="n">pointer_kwargs</span> <span class="o">=</span> <span class="n">pointerize_args_and_kwargs</span><span class="p">(</span>
                <span class="n">args</span><span class="o">=</span><span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">downcast_kwargs</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
                <span class="n">gc_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">cmd</span> <span class="o">=</span> <span class="n">RunClassMethodAction</span><span class="p">(</span>
                <span class="n">path</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
                <span class="n">_self</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="n">pointer_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">pointer_kwargs</span><span class="p">,</span>
                <span class="n">id_at_location</span><span class="o">=</span><span class="n">result_id_at_location</span><span class="p">,</span>
                <span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">address</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">send_immediate_msg_without_reply</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="n">cmd</span><span class="p">)</span>

        <span class="n">inherit_tags</span><span class="p">(</span>
            <span class="n">attr_path_and_name</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
            <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span>
            <span class="n">self_obj</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">other</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>

        <span class="n">result_public_shape</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">):</span>
            <span class="n">other_shape</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">public_shape</span>
            <span class="n">other_dtype</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">public_dtype</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="n">other_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">other_dtype</span> <span class="o">=</span> <span class="n">DEFAULT_INT_NUMPY_TYPE</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="n">other_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">other_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s2">&quot;bool&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">other_shape</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">other_dtype</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid Type for TensorWrappedGammaTensorPointer:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">other_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">result_public_shape</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(</span>
                <span class="n">op_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">other_shape</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">other_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span> <span class="o">!=</span> <span class="n">other_dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Dtype for self: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span><span class="si">}</span><span class="s2"> and other :</span><span class="si">{</span><span class="n">other_dtype</span><span class="si">}</span><span class="s2"> should not be None&quot;</span>
                <span class="p">)</span>

        <span class="c1"># calculate the dtype of the result based on the op_str</span>
        <span class="n">result_public_dtype</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_dtype</span><span class="p">(</span>
            <span class="n">op_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">other_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span><span class="p">,</span> <span class="n">other_dtype</span>
        <span class="p">)</span>

        <span class="n">result</span><span class="o">.</span><span class="n">public_shape</span> <span class="o">=</span> <span class="n">result_public_shape</span>
        <span class="n">result</span><span class="o">.</span><span class="n">public_dtype</span> <span class="o">=</span> <span class="n">result_public_dtype</span>

        <span class="n">result</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">processing_pointers</span><span class="p">[</span><span class="n">result</span><span class="o">.</span><span class="n">id_at_location</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_apply_op</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">:</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
        <span class="n">op_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MPCTensor</span><span class="p">,</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Performs the operation based on op_str</span>

<span class="sd">        Args:</span>
<span class="sd">            other (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]): second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[MPCTensor,Union[MPCTensor,int,float,np.ndarray]] : Result of the operation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># relative</span>
        <span class="kn">from</span> <span class="nn">..autodp.phi_tensor</span> <span class="kn">import</span> <span class="n">TensorWrappedPhiTensorPointer</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">TensorWrappedPhiTensorPointer</span><span class="p">):</span>
            <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">gamma</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">client</span>
        <span class="p">):</span>

            <span class="n">parties</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">client</span><span class="p">]</span>

            <span class="n">self_mpc</span> <span class="o">=</span> <span class="n">MPCTensor</span><span class="p">(</span><span class="n">secret</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">parties</span><span class="o">=</span><span class="n">parties</span><span class="p">)</span>
            <span class="n">other_mpc</span> <span class="o">=</span> <span class="n">MPCTensor</span><span class="p">(</span>
                <span class="n">secret</span><span class="o">=</span><span class="n">other</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">other</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">parties</span><span class="o">=</span><span class="n">parties</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">self_mpc</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="n">other_mpc</span><span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">):</span>

            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">other</span><span class="o">=</span><span class="n">other</span><span class="p">,</span> <span class="n">op_str</span><span class="o">=</span><span class="n">op_str</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_apply_self_tensor_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># we want to get the return type which matches the attr_path_and_name</span>
        <span class="c1"># so we ask lib_ast for the return type name that matches out</span>
        <span class="c1"># attr_path_and_name and then use that to get the actual pointer klass</span>
        <span class="c1"># then set the result to that pointer klass</span>

        <span class="c1"># We always maintain a Tensor hierarchy Tensor ---&gt; PT--&gt; Actual Data</span>
        <span class="n">attr_path_and_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;syft.core.tensor.tensor.Tensor.</span><span class="si">{</span><span class="n">op_str</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">(</span>
            <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># QUESTION can the id_at_location be None?</span>
        <span class="n">result_id_at_location</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s2">&quot;id_at_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result_id_at_location</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># first downcast anything primitive which is not already PyPrimitive</span>
            <span class="p">(</span>
                <span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">downcast_kwargs</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">downcast_args_and_kwargs</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># then we convert anything which isnt a pointer into a pointer</span>
            <span class="n">pointer_args</span><span class="p">,</span> <span class="n">pointer_kwargs</span> <span class="o">=</span> <span class="n">pointerize_args_and_kwargs</span><span class="p">(</span>
                <span class="n">args</span><span class="o">=</span><span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">downcast_kwargs</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
                <span class="n">gc_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">cmd</span> <span class="o">=</span> <span class="n">RunClassMethodAction</span><span class="p">(</span>
                <span class="n">path</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
                <span class="n">_self</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="n">pointer_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">pointer_kwargs</span><span class="p">,</span>
                <span class="n">id_at_location</span><span class="o">=</span><span class="n">result_id_at_location</span><span class="p">,</span>
                <span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">address</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">send_immediate_msg_without_reply</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="n">cmd</span><span class="p">)</span>

        <span class="n">inherit_tags</span><span class="p">(</span>
            <span class="n">attr_path_and_name</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
            <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span>
            <span class="n">self_obj</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># relative</span>
        <span class="kn">from</span> <span class="nn">..autodp.phi_tensor</span> <span class="kn">import</span> <span class="n">TensorWrappedPhiTensorPointer</span>

        <span class="k">if</span> <span class="n">op_str</span> <span class="o">==</span> <span class="s2">&quot;choose&quot;</span><span class="p">:</span>
            <span class="n">dummy_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="p">(</span><span class="n">TensorWrappedPhiTensorPointer</span><span class="p">,</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">),</span>
            <span class="p">):</span>
                <span class="n">temp_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                <span class="n">dummy_res</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">dummy_res</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="o">*</span><span class="n">temp_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dummy_res</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">dummy_res</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dummy_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dummy_res</span><span class="p">,</span> <span class="n">op_str</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">op_str</span> <span class="ow">in</span> <span class="n">INPLACE_OPS</span><span class="p">:</span>
                    <span class="nb">getattr</span><span class="p">(</span><span class="n">dummy_res</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dummy_res</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">dummy_res</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">np</span><span class="p">,</span> <span class="n">op_str</span><span class="p">):</span>
                <span class="n">dummy_res</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">np</span><span class="p">,</span> <span class="n">op_str</span><span class="p">)(</span><span class="n">dummy_res</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid Numpy Operation: </span><span class="si">{</span><span class="n">op_str</span><span class="si">}</span><span class="s2"> for Pointer&quot;</span><span class="p">)</span>

        <span class="n">result</span><span class="o">.</span><span class="n">public_shape</span> <span class="o">=</span> <span class="n">dummy_res</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">result</span><span class="o">.</span><span class="n">public_dtype</span> <span class="o">=</span> <span class="n">dummy_res</span><span class="o">.</span><span class="n">dtype</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;copy&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;add&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__add__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;radd&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__radd__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;sub&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__sub__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;rsub&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__rsub__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;mul&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__mul__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;rmul&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__rmul__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__matmul__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;matmul&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__matmul__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;rmatmul&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__rmatmul__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;lt&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__lt__&quot;)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;gt&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__gt__&quot;)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;ge&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__ge__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;le&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__le__&quot;)</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;eq&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__eq__&quot;)</span>

    <span class="k">def</span> <span class="fm">__ne__</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;ne&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__ne__&quot;)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.concatenate"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.concatenate">[docs]</a>    <span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MPCTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;concatenate&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>


<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Concatenate works only for TensorWrappedGammaTensorPointer got type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">client</span><span class="p">:</span>

            <span class="n">parties</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">client</span><span class="p">]</span>

            <span class="n">self_mpc</span> <span class="o">=</span> <span class="n">MPCTensor</span><span class="p">(</span><span class="n">secret</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">parties</span><span class="o">=</span><span class="n">parties</span><span class="p">)</span>
            <span class="n">other_mpc</span> <span class="o">=</span> <span class="n">MPCTensor</span><span class="p">(</span>
                <span class="n">secret</span><span class="o">=</span><span class="n">other</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">other</span><span class="o">.</span><span class="n">public_shape</span><span class="p">,</span> <span class="n">parties</span><span class="o">=</span><span class="n">parties</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">self_mpc</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">other_mpc</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Concatenate method currently works only between two different clients.&quot;</span>
            <span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;truediv&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__truediv__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;rtruediv&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__rtruediv__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__mod__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__mod__&quot;)</span>

    <span class="k">def</span> <span class="fm">__and__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;and&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__and__&quot;)</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;or&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__or__&quot;)</span>

    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;floordiv&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__floordiv__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;rfloordiv&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__rfloordiv__&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__divmod__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">],</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;divmod&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">divmod</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.divmod"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.divmod">[docs]</a>    <span class="k">def</span> <span class="nf">divmod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">],</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;divmod&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__floordiv__&quot;</span>
        <span class="p">),</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="o">.</span><span class="n">_apply_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;__mod__&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.sum"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.sum">[docs]</a>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sum of array elements over a given axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which a sum is performed.</span>
<span class="sd">                The default, axis=None, will sum all of the elements of the input array.</span>
<span class="sd">                If axis is negative it counts from the last to the first axis.</span>
<span class="sd">                If axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a</span>
<span class="sd">                single axis or all the axes as before.</span>
<span class="sd">            keepdims: bool, optional</span>
<span class="sd">                If this is set to True, the axes which are reduced are left in the result as dimensions with size one.</span>
<span class="sd">                With this option, the result will broadcast correctly against the input array.</span>
<span class="sd">                If the default value is passed, then keepdims will not be passed through to the sum method of</span>
<span class="sd">                sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not</span>
<span class="sd">                implement keepdims any exceptions will be raised.</span>
<span class="sd">            initial: scalar, optional</span>
<span class="sd">                Starting value for the sum. See reduce for details.</span>
<span class="sd">            where: array_like of bool, optional</span>
<span class="sd">                Elements to include in the sum. See reduce for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.ptp"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.ptp">[docs]</a>    <span class="k">def</span> <span class="nf">ptp</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;ptp&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;ptp&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__lshift__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;lshift&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__lshift__&quot;)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.argmax"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.argmax">[docs]</a>    <span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;argmax&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
        <span class="c1"># return self._apply_self_tensor_op(&quot;argmax&quot;, *args, **kwargs)</span>

    <span class="k">def</span> <span class="fm">__rshift__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;rshift&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__rshift__&quot;)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.argmin"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.argmin">[docs]</a>    <span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;argmin&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
        <span class="c1"># return self._apply_self_tensor_op(&quot;argmin&quot;, *args, **kwargs)</span>

    <span class="k">def</span> <span class="fm">__abs__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;abs&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;__abs__&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.all"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.all">[docs]</a>    <span class="k">def</span> <span class="nf">all</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;all&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
        <span class="c1"># return self._apply_self_tensor_op(&quot;all&quot;, *args, **kwargs)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.any"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.any">[docs]</a>    <span class="k">def</span> <span class="nf">any</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;any&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
        <span class="c1"># return self._apply_self_tensor_op(&quot;any&quot;, *args, **kwargs)</span>

    <span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;round&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__round__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__pos__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the pos (+) operator  on self.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="n">op_str</span><span class="o">=</span><span class="s2">&quot;__pos__&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.var"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.var">[docs]</a>    <span class="k">def</span> <span class="nf">var</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the variance along the specified axis of the array elements, a measure of the spread of a distribution.</span>
<span class="sd">        The variance is computed for the flattened array by default, otherwise over the specified axis.</span>

<span class="sd">        Parameters</span>

<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which the variance is computed.</span>
<span class="sd">                The default is to compute the variance of the flattened array.</span>
<span class="sd">                If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all</span>
<span class="sd">                the axes as before.</span>

<span class="sd">            ddof: int, optional</span>
<span class="sd">                “Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the</span>
<span class="sd">                number of elements. By default ddof is zero.</span>

<span class="sd">            keepdims: bool, optional</span>
<span class="sd">                If this is set to True, the axes which are reduced are left in the result as dimensions with size one.</span>
<span class="sd">                With this option, the result will broadcast correctly against the input array.</span>
<span class="sd">                If the default value is passed, then keepdims will not be passed through to the var method of</span>
<span class="sd">                sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not</span>
<span class="sd">                implement keepdims any exceptions will be raised.</span>

<span class="sd">            where: array_like of bool, optional</span>
<span class="sd">                Elements to include in the variance. See reduce for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;var&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.cumsum"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.cumsum">[docs]</a>    <span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; &quot;</span>
<span class="sd">        Return the cumulative sum of the elements along a given axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: int, optional</span>
<span class="sd">                Axis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the</span>
<span class="sd">                flattened array.</span>
<span class="sd">        Returns</span>
<span class="sd">            cumsum_along_axis: PhiTensor</span>
<span class="sd">                A new array holding the result is returned. The result has the same size as input, and the same shape as</span>
<span class="sd">                 a if axis is not None or a is 1-d.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;cumsum&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.cumprod"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.cumprod">[docs]</a>    <span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the cumulative product of the elements along a given axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: int, optional</span>
<span class="sd">                Axis along which the cumulative product is computed. The default (None) is to compute the cumprod over</span>
<span class="sd">                the flattened array.</span>
<span class="sd">        Returns</span>
<span class="sd">            cumprod_along_axis: PhiTensor</span>
<span class="sd">                A new array holding the result is returned. The result has the same size as input, and the same shape as</span>
<span class="sd">                 a if axis is not None or a is 1-d.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;cumprod&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.prod"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.prod">[docs]</a>    <span class="k">def</span> <span class="nf">prod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the product of array elements over a given axis.</span>
<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which a product is performed.</span>
<span class="sd">                The default, axis=None, will calculate the product of all the elements in the input array.</span>
<span class="sd">                If axis is negative it counts from the last to the first axis.</span>
<span class="sd">                If axis is a tuple of ints, a product is performed on all of the axes specified in the tuple instead of</span>
<span class="sd">                a single axis or all the axes as before.</span>
<span class="sd">            keepdims: bool, optional</span>
<span class="sd">                If this is set to True, the axes which are reduced are left in the result as dimensions with size one.</span>
<span class="sd">                With this option, the result will broadcast correctly against the input array.</span>
<span class="sd">                If the default value is passed, then keepdims will not be passed through to the prod method of</span>
<span class="sd">                sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not</span>
<span class="sd">                implement keepdims any exceptions will be raised.</span>
<span class="sd">            initial: scalar, optional</span>
<span class="sd">                The starting value for this product. See reduce for details.</span>
<span class="sd">            where: array_like of bool, optional</span>
<span class="sd">                Elements to include in the product. See reduce for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;prod&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__xor__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
        <span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;xor&quot; operation between &quot;self&quot; and &quot;other&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return TensorWrappedGammaTensorPointer._apply_op(self, other, &quot;__xor__&quot;)</span>

    <span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        First array elements raised to powers from second array, element-wise.</span>

<span class="sd">        Raise each base in x1 to the positionally-corresponding power in x2.</span>
<span class="sd">        x1 and x2 must be broadcastable to the same shape.</span>
<span class="sd">        An integer type raised to a negative integer power will raise a ValueError.</span>
<span class="sd">        Negative values raised to a non-integral value will return nan.</span>

<span class="sd">        Parameters</span>
<span class="sd">            x2: array_like</span>

<span class="sd">                The exponents. If self.shape != x2.shape, they must be broadcastable to a common shape.</span>

<span class="sd">            where: array_like, optional</span>

<span class="sd">                This condition is broadcast over the input. At locations where the condition is True, the out array will</span>
<span class="sd">                 be set to the ufunc result.</span>
<span class="sd">                 Elsewhere, the out array will retain its original value.</span>

<span class="sd">            **kwargs</span>
<span class="sd">                For other keyword-only arguments, see the ufunc docs.</span>

<span class="sd">        Returns</span>
<span class="sd">            y: PhiTensorPointer</span>
<span class="sd">                The bases in the tensor raised to the exponents in x2. This is a scalar if both self and x2 are scalars.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;__pow__&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.mean"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.mean">[docs]</a>    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the arithmetic mean along the specified axis.</span>

<span class="sd">        Returns the average of the array elements. The average is taken over the flattened array by default, otherwise</span>
<span class="sd">        over the specified axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which the means are computed. The default is to compute the mean of the flattened</span>
<span class="sd">                array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.std"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.std">[docs]</a>    <span class="k">def</span> <span class="nf">std</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the standard deviation along the specified axis.</span>
<span class="sd">        Returns the standard deviation, a measure of the spread of a distribution, of the array elements.</span>
<span class="sd">        The standard deviation is computed for the flattened array by default, otherwise over the specified axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which the standard deviation is computed.</span>
<span class="sd">                The default is to compute the standard deviation of the flattened array.</span>
<span class="sd">                If this is a tuple of ints, a standard deviation is performed over multiple axes, instead of a single</span>
<span class="sd">                axis or all the axes as before.</span>

<span class="sd">            out: ndarray, optional</span>
<span class="sd">                Alternative output array in which to place the result. It must have the same shape as the expected</span>
<span class="sd">                output but the type (of the calculated values) will be cast if necessary.</span>

<span class="sd">            ddof: int, optional</span>
<span class="sd">                ddof = Delta Degrees of Freedom. By default ddof is zero.</span>
<span class="sd">                The divisor used in calculations is N - ddof, where N represents the number of elements.</span>

<span class="sd">            keepdims: bool, optional</span>
<span class="sd">                If this is set to True, the axes which are reduced are left in the result as dimensions with size one.</span>
<span class="sd">                With this option, the result will broadcast correctly against the input array.</span>

<span class="sd">                If the default value is passed, then keepdims will not be passed through to the std method of</span>
<span class="sd">                sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not</span>
<span class="sd">                implement keepdims any exceptions will be raised.</span>

<span class="sd">            where: array_like of bool, optional</span>
<span class="sd">                Elements to include in the standard deviation. See reduce for details.</span>

<span class="sd">        Returns</span>

<span class="sd">            standard_deviation: PhiTensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">attr_path_and_name</span> <span class="o">=</span> <span class="s2">&quot;syft.core.tensor.tensor.Tensor.std&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">(</span>
            <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># QUESTION can the id_at_location be None?</span>
        <span class="n">result_id_at_location</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s2">&quot;id_at_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result_id_at_location</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># first downcast anything primitive which is not already PyPrimitive</span>
            <span class="p">(</span>
                <span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">downcast_kwargs</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">downcast_args_and_kwargs</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># then we convert anything which isnt a pointer into a pointer</span>
            <span class="n">pointer_args</span><span class="p">,</span> <span class="n">pointer_kwargs</span> <span class="o">=</span> <span class="n">pointerize_args_and_kwargs</span><span class="p">(</span>
                <span class="n">args</span><span class="o">=</span><span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">downcast_kwargs</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
                <span class="n">gc_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">cmd</span> <span class="o">=</span> <span class="n">RunClassMethodAction</span><span class="p">(</span>
                <span class="n">path</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
                <span class="n">_self</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="n">pointer_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">pointer_kwargs</span><span class="p">,</span>
                <span class="n">id_at_location</span><span class="o">=</span><span class="n">result_id_at_location</span><span class="p">,</span>
                <span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">address</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">send_immediate_msg_without_reply</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="n">cmd</span><span class="p">)</span>

        <span class="n">inherit_tags</span><span class="p">(</span>
            <span class="n">attr_path_and_name</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
            <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span>
            <span class="n">self_obj</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>
        <span class="n">result</span><span class="o">.</span><span class="n">public_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># data_subjects.shape</span>
        <span class="n">result</span><span class="o">.</span><span class="n">public_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span>

        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.trace"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.trace">[docs]</a>    <span class="k">def</span> <span class="nf">trace</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the sum along diagonals of the array.</span>

<span class="sd">        If a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements</span>
<span class="sd">        a[i,i+offset] for all i.</span>

<span class="sd">        If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D</span>
<span class="sd">        sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and</span>
<span class="sd">        axis2 removed.</span>

<span class="sd">        Parameters</span>

<span class="sd">            offset: int, optional</span>
<span class="sd">                Offset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.</span>

<span class="sd">            axis1, axis2: int, optional</span>
<span class="sd">                Axes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be</span>
<span class="sd">                taken. Defaults are the first two axes of a.</span>

<span class="sd">        Returns</span>

<span class="sd">            Union[TensorWrappedPhiTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">                If a is 2-D, the sum along the diagonal is returned.</span>
<span class="sd">                If a has larger dimensions, then an array of sums along diagonals is returned.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;trace&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.sort"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.sort">[docs]</a>    <span class="k">def</span> <span class="nf">sort</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a sorted copy of an array.</span>

<span class="sd">        Parameters</span>

<span class="sd">            a: array_like</span>
<span class="sd">                Array to be sorted.</span>

<span class="sd">            axis: int or None, optional</span>
<span class="sd">                Axis along which to sort. If None, the array is flattened before sorting.</span>
<span class="sd">                The default is -1, which sorts along the last axis.</span>

<span class="sd">            kind{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional</span>
<span class="sd">                Sorting algorithm. The default is ‘quicksort’.</span>
<span class="sd">                Note that both ‘stable’ and ‘mergesort’ use timsort or radix sort under the covers and, in general,</span>
<span class="sd">                the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards</span>
<span class="sd">                compatibility.</span>

<span class="sd">                Changed in version 1.15.0.: The ‘stable’ option was added.</span>

<span class="sd">            order: str or list of str, optional</span>
<span class="sd">                When a is an array with fields defined, this argument specifies which fields to compare first, second,</span>
<span class="sd">                etc. A single field can be specified as a string, and not all fields need be specified, but unspecified</span>
<span class="sd">                 fields will still be used, in the order in which they come up in the dtype, to break ties.</span>

<span class="sd">        Please see docs here: https://numpy.org/doc/stable/reference/generated/numpy.sort.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;sort&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.argsort"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.argsort">[docs]</a>    <span class="k">def</span> <span class="nf">argsort</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the indices that would sort an array.</span>

<span class="sd">        Perform an indirect sort along the given axis using the algorithm specified by the kind keyword.</span>
<span class="sd">        It returns an array of indices of the same shape as a that index data along the given axis in sorted order.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: int or None, optional</span>
<span class="sd">                Axis along which to sort. The default is -1 (the last axis). If None, the flattened array is used.</span>
<span class="sd">            kind: {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional</span>
<span class="sd">                Sorting algorithm. The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort</span>
<span class="sd">                under the covers and, in general, the actual implementation will vary with data type. The ‘mergesort’</span>
<span class="sd">                option is retained for backwards compatibility.</span>
<span class="sd">            order: str or list of str, optional</span>
<span class="sd">                When a is an array with fields defined, this argument specifies which fields to compare 1st, 2nd, etc.</span>
<span class="sd">                A single field can be specified as a string, and not all fields need be specified, but unspecified</span>
<span class="sd">                fields will still be used, in the order in which they come up in the dtype, to break ties.</span>

<span class="sd">        Returns</span>
<span class="sd">            index_array: ndarray, int</span>
<span class="sd">                Array of indices that sort a along the specified axis. If a is one-dimensional, a[index_array] yields a</span>
<span class="sd">                sorted a. More generally, np.take_along_axis(a, index_array, axis=axis) always yields the sorted a,</span>
<span class="sd">                irrespective of dimensionality.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
        <span class="c1"># return self._apply_self_tensor_op(&quot;argsort&quot;, *args, **kwargs)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.min"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.min">[docs]</a>    <span class="k">def</span> <span class="nf">min</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the minimum of an array or minimum along an axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which to operate. By default, flattened input is used.</span>
<span class="sd">                If this is a tuple of ints, the minimum is selected over multiple axes,</span>
<span class="sd">                instead of a single axis or all the axes as before.</span>

<span class="sd">        Returns</span>
<span class="sd">            a_min: PhiTensor</span>
<span class="sd">                Minimum of a.</span>
<span class="sd">                If axis is None, the result is a scalar value.</span>
<span class="sd">                If axis is given, the result is an array of dimension a.ndim - 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.max"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.max">[docs]</a>    <span class="k">def</span> <span class="nf">max</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the maximum of an array or along an axis.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Axis or axes along which to operate. By default, flattened input is used.</span>
<span class="sd">                If this is a tuple of ints, the minimum is selected over multiple axes,</span>
<span class="sd">                instead of a single axis or all the axes as before.</span>

<span class="sd">        Returns</span>
<span class="sd">            a_max: PhiTensor</span>
<span class="sd">                Maximum of a.</span>
<span class="sd">                If axis is None, the result is a scalar value.</span>
<span class="sd">                If axis is given, the result is an array of dimension a.ndim - 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.compress"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.compress">[docs]</a>    <span class="k">def</span> <span class="nf">compress</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return selected slices of an array along given axis.</span>

<span class="sd">        When working along a given axis, a slice along that axis is returned in output for each index</span>
<span class="sd">        where condition evaluates to True. When working on a 1-D array, compress is equivalent to extract.</span>

<span class="sd">        Parameters</span>
<span class="sd">            condition: 1-D array of bools</span>
<span class="sd">            Array that selects which entries to return. If len(condition) is less than the size of</span>
<span class="sd">            a along the given axis,then output is truncated to the length of the condition array.</span>

<span class="sd">            axis: int, optional</span>
<span class="sd">            Axis along which to take slices. If None (default), work on the flattened array.</span>

<span class="sd">        Returns:</span>
<span class="sd">            compressed_array: PhiTensor</span>
<span class="sd">            A copy of a without the slices along axis for which condition is false.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;compress&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.squeeze"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.squeeze">[docs]</a>    <span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove axes of length one from a.</span>

<span class="sd">        Parameters</span>
<span class="sd">            axis: None or int or tuple of ints, optional</span>
<span class="sd">                Selects a subset of the entries of length one in the shape.</span>
<span class="sd">                If an axis is selected with shape entry greater than one, an error is raised.</span>

<span class="sd">        Returns:</span>
<span class="sd">            squeezed: PhiTensor</span>
<span class="sd">                The input array, but with all or a subset of the dimensions of length 1 removed.</span>
<span class="sd">                This is always a itself or a view into a.</span>
<span class="sd">                Note that if all axes are squeezed, the result is a 0d array and not a scalar.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;squeeze&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">slice</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return self[key].</span>
<span class="sd">        Args:</span>
<span class="sd">            y (Union[int,bool,slice]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.zeros_like"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.zeros_like">[docs]</a>    <span class="k">def</span> <span class="nf">zeros_like</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;zeros_like&quot; operation on &quot;self&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;zeros_like&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.ones_like"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.ones_like">[docs]</a>    <span class="k">def</span> <span class="nf">ones_like</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the &quot;ones_like&quot; operation on &quot;self&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedGammaTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedGammaTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;ones_like&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.transpose"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.transpose">[docs]</a>    <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverse or permute the axes of an array; returns the modified array.</span>

<span class="sd">        Returns</span>
<span class="sd">            p: ndarray</span>
<span class="sd">                array with its axes permuted. A view is returned whenever possible.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;transpose&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.resize"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.resize">[docs]</a>    <span class="k">def</span> <span class="nf">resize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a new array with the specified shape.</span>

<span class="sd">        Parameters</span>
<span class="sd">            new_shape: int or tuple of int</span>
<span class="sd">                Shape of resized array.</span>

<span class="sd">        Returns</span>
<span class="sd">            reshaped_array: ndarray</span>
<span class="sd">                The new array is formed from the data in the old array,</span>
<span class="sd">                repeated if necessary to fill out the required number of elements.</span>
<span class="sd">                The data are repeated iterating over the array in C-order.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;resize&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.reshape"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.reshape">[docs]</a>    <span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gives a new shape to an array without changing its data.</span>

<span class="sd">        Parameters</span>
<span class="sd">            new_shape: int or tuple of int</span>
<span class="sd">                The new shape should be compatible with the original shape. If an integer, then the result will</span>
<span class="sd">                be a 1-D array of that length. One shape dimension can be -1. In this case,</span>
<span class="sd">                the value is inferred from the length of the array and remaining dimensions.</span>

<span class="sd">        Returns</span>
<span class="sd">            reshaped_array: ndarray</span>
<span class="sd">                This will be a new view object if possible; otherwise, it will be a copy.</span>
<span class="sd">                Note there is no guarantee of the memory layout (C- or Fortran- contiguous) of the returned array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;reshape&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.repeat"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.repeat">[docs]</a>    <span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the repeat&quot; operation</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[TensorWrappedPhiTensorPointer,MPCTensor,int,float,np.ndarray]) : second operand.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedPhiTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;repeat&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.diagonal"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.diagonal">[docs]</a>    <span class="k">def</span> <span class="nf">diagonal</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return specified diagonals.</span>
<span class="sd">        If a is 2-D, returns the diagonal of a with the given offset, i.e., the collection of elements</span>
<span class="sd">        of the form a[i, i+offset].</span>

<span class="sd">        If a has more than two dimensions, then the axes specified by axis1 and axis are used to determine</span>
<span class="sd">        the 2-D sub-array whose diagonal is returned.  The shape of the resulting array can be determined by</span>
<span class="sd">        removing axis1 and axis2 and appending an index to the right equal to the size of the resulting diagonals.</span>

<span class="sd">        Parameters</span>

<span class="sd">            offset: int, optional</span>
<span class="sd">                Offset of the diagonal from the main diagonal.  Can be positive or negative.</span>
<span class="sd">                Defaults to main diagonal (0).</span>
<span class="sd">            axis1, axis2: int, optional</span>
<span class="sd">                Axis to be used as the first axis of the 2-D sub-arrays from which the diagonals should be taken.</span>
<span class="sd">                Defaults are the first two axes of a.</span>

<span class="sd">        Returns</span>
<span class="sd">            array_of_diagonals : Union[TensorWrappedPhiTensorPointer,MPCTensor]</span>
<span class="sd">                If a is 2-D, then a 1-D array containing the diagonal and of the same type as a is returned unless</span>
<span class="sd">                a is a matrix, in which case</span>
<span class="sd">                a 1-D array rather than a (2-D) matrix is returned in order to maintain backward compatibility.</span>

<span class="sd">                If a.ndim &gt; 2, then the dimensions specified by axis1 and axis2 are removed, and a new axis</span>
<span class="sd">                inserted at the end corresponding to the diagonal.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;diagonal&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.flatten"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.flatten">[docs]</a>    <span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a copy of the array collapsed into one dimension.</span>

<span class="sd">        Parameters</span>
<span class="sd">            order: {‘C’, ‘F’, ‘A’, ‘K’}, optional</span>
<span class="sd">            ‘C’ means to flatten in row-major (C-style) order.</span>
<span class="sd">            ‘F’ means to flatten in column-major (Fortran- style) order.</span>
<span class="sd">            ‘A’ means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise.</span>
<span class="sd">            ‘K’ means to flatten a in the order the elements occur in memory. The default is ‘C’.</span>

<span class="sd">        Returns</span>
<span class="sd">            y: PhiTensor</span>
<span class="sd">                A copy of the input array, flattened to one dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;flatten&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.ravel"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.ravel">[docs]</a>    <span class="k">def</span> <span class="nf">ravel</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a contiguous flattened array.</span>

<span class="sd">        A 1-D array, containing the elements of the input, is returned. A copy is made only if needed.</span>

<span class="sd">        As of NumPy 1.10, the returned array will have the same type as the input array.</span>
<span class="sd">        (for example, a masked array will be returned for a masked array input)</span>
<span class="sd">        Parameters</span>
<span class="sd">            order: {‘C’,’F’, ‘A’, ‘K’}, optional</span>
<span class="sd">            The elements of a are read using this index order.</span>
<span class="sd">            ‘C’ means to index the elements in row-major,</span>
<span class="sd">            C-style order, with the last axis index changing fastest, back to the first axis index changing slowest.</span>
<span class="sd">            ‘F’ means to index the elements in column-major, Fortran-style order, with the first index changing fastest,</span>
<span class="sd">             and the last index changing slowest.</span>
<span class="sd">            Note that the ‘C’ and ‘F’ options take no account of the memory layout of the underlying array,</span>
<span class="sd">             and only refer to the order of axis indexing.</span>
<span class="sd">            ‘A’ means to read the elements in Fortran-like index order if a is Fortran contiguous in memory,</span>
<span class="sd">             C-like order otherwise.</span>
<span class="sd">            ‘K’ means to read the elements in the order they occur in memory, except for reversing the data</span>
<span class="sd">             when strides are negative.</span>
<span class="sd">            By default, ‘C’ index order is used.</span>

<span class="sd">        Returns:</span>
<span class="sd">            y: PhiTensor</span>
<span class="sd">                y is an array of the same subtype as a, with shape (a.size,).</span>
<span class="sd">                Note that matrices are special cased for backward compatibility,</span>
<span class="sd">                if a is a matrix, then y is a 1-D ndarray.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;ravel&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.take"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.take">[docs]</a>    <span class="k">def</span> <span class="nf">take</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Take elements from an array along an axis.</span>

<span class="sd">        When axis is not None, this function does the same thing as “fancy” indexing (indexing arrays using arrays);</span>
<span class="sd">        however, it can be easier to use if you need elements along a given axis.</span>
<span class="sd">        A call such as np.take(arr, indices, axis=3) is equivalent to arr[:,:,:,indices,...].</span>

<span class="sd">        Explained without fancy indexing, this is equivalent to the following use of ndindex, \</span>
<span class="sd">        which sets each of ii, jj, and kk to a tuple of indices:</span>

<span class="sd">            Ni, Nk = a.shape[:axis], a.shape[axis+1:]</span>
<span class="sd">            Nj = indices.shape</span>
<span class="sd">            for ii in ndindex(Ni):</span>
<span class="sd">                for jj in ndindex(Nj):</span>
<span class="sd">                    for kk in ndindex(Nk):</span>
<span class="sd">                        out[ii + jj + kk] = a[ii + (indices[jj],) + kk]</span>

<span class="sd">        Parameters</span>
<span class="sd">            indices: array_like (Nj…)</span>
<span class="sd">                The indices of the values to extract.</span>

<span class="sd">            axis: int, optional</span>
<span class="sd">                The axis over which to select values. By default, the flattened input array is used.</span>

<span class="sd">            mode: {‘raise’, ‘wrap’, ‘clip’}, optional</span>
<span class="sd">                Specifies how out-of-bounds indices will behave.</span>

<span class="sd">                * ‘raise’ – raise an error (default)</span>

<span class="sd">                * ‘wrap’ – wrap around</span>

<span class="sd">                * ‘clip’ – clip to the range</span>

<span class="sd">                ‘clip’ mode means that all indices that are too large are replaced by the index</span>
<span class="sd">                that addresses the last element along that axis.</span>
<span class="sd">                Note that this disables indexing with negative numbers.</span>

<span class="sd">        Returns</span>
<span class="sd">            out: PhiTensor</span>
<span class="sd">                The returned array has the same type as a.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;take&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.clip"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.clip">[docs]</a>    <span class="k">def</span> <span class="nf">clip</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clip (limit) the values in an array.</span>

<span class="sd">        Parameters</span>
<span class="sd">            a : array_like</span>
<span class="sd">                Array containing elements to clip.</span>
<span class="sd">            a_min, a_max : array_like or None</span>
<span class="sd">                Minimum and maximum value. If None, clipping is not performed on</span>
<span class="sd">                the corresponding edge. Only one of a_min and a_max may be</span>
<span class="sd">                None. Both are broadcast against a.</span>
<span class="sd">        Returns:</span>
<span class="sd">            Union[TensorWrappedPhiTensorPointer,MPCTensor] : Result of the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;clip&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.choose"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.choose">[docs]</a>    <span class="k">def</span> <span class="nf">choose</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">MPCTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Construct an array from an index array and a list of arrays to choose from.</span>

<span class="sd">        First of all, if confused or uncertain, definitely look at the Examples - in its full generality,</span>
<span class="sd">        this function is less simple than it might seem from the following code description</span>
<span class="sd">        (below ndi = numpy.lib.index_tricks):</span>

<span class="sd">        np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)]).</span>

<span class="sd">        But this omits some subtleties. Here is a fully general summary:</span>

<span class="sd">        Given an “index” array (a) of integers and a sequence of n arrays (choices), a and each choice array are first</span>
<span class="sd">        broadcast, as necessary, to arrays of a common shape; calling these Ba and Bchoices[i], i = 0,…,n-1 we have that</span>
<span class="sd">         necessarily, Ba.shape == Bchoices[i].shape for each i. Then, a new array with shape Ba.shape is created</span>
<span class="sd">         as follows:</span>

<span class="sd">            if mode=&#39;raise&#39; (the default), then, first of all, each element of a (and thus Ba) must be in the range</span>
<span class="sd">            [0, n-1]; now, suppose that i (in that range) is the value at the (j0, j1, ..., jm) position in Ba -</span>
<span class="sd">            then the value at the same position in the new array is the value in Bchoices[i] at that same position;</span>

<span class="sd">            if mode=&#39;wrap&#39;, values in a (and thus Ba) may be any (signed) integer; modular arithmetic is used to map</span>
<span class="sd">            integers outside the range [0, n-1] back into that range; and then the new array is constructed as above;</span>

<span class="sd">            if mode=&#39;clip&#39;, values in a (and thus Ba) may be any (signed) integer; negative integers are mapped to 0;</span>
<span class="sd">            values greater than n-1 are mapped to n-1; and then the new array is constructed as above.</span>

<span class="sd">        Parameters</span>

<span class="sd">            choices: sequence of arrays</span>

<span class="sd">                Choice arrays. a and all of the choices must be broadcastable to the same shape. If choices is itself an</span>
<span class="sd">                 array (not recommended), then its outermost dimension (i.e., the one corresponding to choices.shape[0])</span>
<span class="sd">                  is taken as defining the “sequence”.</span>

<span class="sd">            out: array, optional</span>

<span class="sd">                If provided, the result will be inserted into this array. It should be of the appropriate shape and</span>
<span class="sd">                dtype. Note that out is always buffered if mode=&#39;raise&#39;; use other modes for better performance.</span>

<span class="sd">            mode{‘raise’ (default), ‘wrap’, ‘clip’}, optional</span>

<span class="sd">                Specifies how indices outside [0, n-1] will be treated:</span>

<span class="sd">                        ‘raise’ : an exception is raised</span>

<span class="sd">                        ‘wrap’ : value becomes value mod n</span>

<span class="sd">                        ‘clip’ : values &lt; 0 are mapped to 0, values &gt; n-1 are mapped to n-1</span>

<span class="sd">        Returns</span>
<span class="sd">            merged_array: PhiTensor</span>
<span class="sd">                The merged result.</span>

<span class="sd">        Raises</span>
<span class="sd">            ValueError: shape mismatch</span>
<span class="sd">                If a and each choice array are not all broadcastable to the same shape.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_self_tensor_op</span><span class="p">(</span><span class="s2">&quot;choose&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
        <span class="c1"># We always maintain a Tensor hierarchy Tensor ---&gt; PT--&gt; Actual Data</span>
        <span class="n">attr_path_and_name</span> <span class="o">=</span> <span class="s2">&quot;syft.core.tensor.tensor.Tensor.T&quot;</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">(</span>
            <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># QUESTION can the id_at_location be None?</span>
        <span class="n">result_id_at_location</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s2">&quot;id_at_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result_id_at_location</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># first downcast anything primitive which is not already PyPrimitive</span>
            <span class="p">(</span>
                <span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">downcast_kwargs</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">downcast_args_and_kwargs</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[],</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{})</span>

            <span class="c1"># then we convert anything which isnt a pointer into a pointer</span>
            <span class="n">pointer_args</span><span class="p">,</span> <span class="n">pointer_kwargs</span> <span class="o">=</span> <span class="n">pointerize_args_and_kwargs</span><span class="p">(</span>
                <span class="n">args</span><span class="o">=</span><span class="n">downcast_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">downcast_kwargs</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">,</span>
                <span class="n">gc_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">cmd</span> <span class="o">=</span> <span class="n">GetOrSetPropertyAction</span><span class="p">(</span>
                <span class="n">path</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
                <span class="n">id_at_location</span><span class="o">=</span><span class="n">result_id_at_location</span><span class="p">,</span>
                <span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">address</span><span class="p">,</span>
                <span class="n">_self</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="n">pointer_args</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">pointer_kwargs</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">PropertyActions</span><span class="o">.</span><span class="n">GET</span><span class="p">,</span>
                <span class="n">map_to_dyn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">send_immediate_msg_without_reply</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="n">cmd</span><span class="p">)</span>

        <span class="n">inherit_tags</span><span class="p">(</span>
            <span class="n">attr_path_and_name</span><span class="o">=</span><span class="n">attr_path_and_name</span><span class="p">,</span>
            <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span>
            <span class="n">self_obj</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>

        <span class="n">result_public_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">public_shape</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">result</span><span class="o">.</span><span class="n">public_shape</span> <span class="o">=</span> <span class="n">result_public_shape</span>
        <span class="n">result</span><span class="o">.</span><span class="n">public_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">public_dtype</span>

        <span class="k">return</span> <span class="n">result</span>

<div class="viewcode-block" id="TensorWrappedGammaTensorPointer.to_local_object_without_private_data_child"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.TensorWrappedGammaTensorPointer.to_local_object_without_private_data_child">[docs]</a>    <span class="k">def</span> <span class="nf">to_local_object_without_private_data_child</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert this pointer into a partial version of the GammaTensor but without</span>
<span class="sd">        any of the private data therein.&quot;&quot;&quot;</span>
        <span class="c1"># relative</span>
        <span class="kn">from</span> <span class="nn">..tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>

        <span class="n">public_shape</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;public_shape&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">public_dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;public_dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span>
            <span class="n">child</span><span class="o">=</span><span class="n">GammaTensor</span><span class="p">(</span>
                <span class="n">child</span><span class="o">=</span><span class="n">FixedPrecisionTensor</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># TODO 0.7 fix this</span>
                <span class="n">sources</span><span class="o">=</span><span class="p">{},</span>
                <span class="n">jax_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">public_shape</span><span class="o">=</span><span class="n">public_shape</span><span class="p">,</span>
            <span class="n">public_dtype</span><span class="o">=</span><span class="n">public_dtype</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<span class="nd">@implements</span><span class="p">(</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">zeros_like</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="nd">@implements</span><span class="p">(</span><span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ones_like</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorWrappedGammaTensorPointer</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">create_lookup_tables</span><span class="p">(</span><span class="n">dictionary</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]:</span>
    <span class="n">index2key</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
    <span class="n">key2index</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">index2key</span><span class="p">)}</span>
    <span class="c1"># Note this maps to GammaTensor, not to GammaTensor.child as name may imply</span>
    <span class="n">index2values</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index2key</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">index2key</span><span class="p">,</span> <span class="n">key2index</span><span class="p">,</span> <span class="n">index2values</span>


<span class="k">def</span> <span class="nf">create_new_lookup_tables</span><span class="p">(</span>
    <span class="n">dictionary</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Deque</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">Deque</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">Deque</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
    <span class="n">index2key</span><span class="p">:</span> <span class="n">Deque</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
    <span class="n">key2index</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">index2values</span><span class="p">:</span> <span class="n">Deque</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">deque</span><span class="p">()</span>
    <span class="p">)</span>  <span class="c1"># Note this maps to GammaTensor, not to GammaTensor.child as name may imply</span>
    <span class="n">index2size</span><span class="p">:</span> <span class="n">Deque</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">key</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">index2key</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">key2index</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
        <span class="n">index2values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dictionary</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="n">index2size</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">index2key</span><span class="p">,</span> <span class="n">key2index</span><span class="p">,</span> <span class="n">index2values</span><span class="p">,</span> <span class="n">index2size</span>


<span class="k">def</span> <span class="nf">jax2numpy</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
    <span class="c1"># are we incurring copying here?</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">numpy2jax</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="c1"># ATTENTION: Shouldn&#39;t this be a subclass of some kind of base tensor so all the numpy</span>
<span class="c1"># methods and properties don&#39;t need to be re-implemented on it?</span>
<div class="viewcode-block" id="GammaTensor"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.GammaTensor">[docs]</a><span class="nd">@dataclass</span>
<span class="nd">@serializable</span><span class="p">(</span><span class="n">capnp_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">GammaTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A differential privacy tensor that contains data belonging to atleast 2 or more unique data subjects.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        child: jnp.array</span>
<span class="sd">            The private data itself.</span>
<span class="sd">        data_subjects: DataSubjectArray</span>
<span class="sd">            (DP Metadata) A custom NumPy class that keeps track of which data subjects contribute which datapoints in</span>
<span class="sd">            this tensor.</span>
<span class="sd">        min_vals: lazyrepeatarray</span>
<span class="sd">            (DP Metadata) A custom class that keeps track of (data-independent) minimum values for this tensor.</span>
<span class="sd">        max_vals: lazyrepeatarray</span>
<span class="sd">            (DP Metadata) A custom class that keeps track of (data-independent) maximum values for this tensor.</span>
<span class="sd">        func_str: str</span>
<span class="sd">            A string that will determine which function was used to build the current tensor.</span>
<span class="sd">        is_linear: bool</span>
<span class="sd">            Whether the &quot;func_str&quot; for this tensor is a linear query or not. This impacts the epsilon calculations</span>
<span class="sd">            when publishing.</span>
<span class="sd">        sources: dict</span>
<span class="sd">            A dictionary containing all the Tensors, integers, etc that were used to create this tensor.</span>
<span class="sd">            It maps an integer to each input object.</span>
<span class="sd">        id: int</span>
<span class="sd">            A 32-bit integer that is used when this GammaTensor needs to be added to the &quot;sources&quot; dictionary.</span>

<span class="sd">    Methods:</span>
<span class="sd">        All efforts were made to make this tensor&#39;s API as similar to the NumPy API as possible.</span>
<span class="sd">        Special, unique methods are listed below:</span>

<span class="sd">        reconstruct(sources: Optional[dict]):</span>
<span class="sd">            rebuilds the tensor from the sources dictionary provided, or from the current self.sources.</span>
<span class="sd">            This is exclusively used when adding DP Noise, if the data scientist doesn&#39;t have enough privacy budget to</span>
<span class="sd">            use one of the input tensors, thus requiring that tensor&#39;s data to be removed from the computation.</span>

<span class="sd">        swap_state(sources: Optional[Dict]):</span>
<span class="sd">            calls reconstruct() and populates the rest of the GammaTensor&#39;s attributes based on the current tensor.</span>
<span class="sd">            Used exclusively when adding DP Noise.</span>



<span class="sd">        decode():</span>
<span class="sd">            occasionally the use of a FixedPrecisionTensor (FPT) is needed during SMPC[1]. This helps convert back from</span>
<span class="sd">            FPT to regular numpy/jax arrays.</span>

<span class="sd">            (https://en.wikipedia.org/wiki/Secure_multi-party_computation)</span>




<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">PointerClassOverride</span> <span class="o">=</span> <span class="n">TensorWrappedGammaTensorPointer</span>
    <span class="n">__array_ufunc__</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">child</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span>
    <span class="n">jax_op</span><span class="p">:</span> <span class="n">SyftJaxOp</span> <span class="o">=</span> <span class="n">flax</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">pytree_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">sources</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">flax</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">pytree_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">is_linear</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="nb">id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">flax</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">pytree_node</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">default_factory</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">UID</span><span class="p">())</span>

<div class="viewcode-block" id="GammaTensor.decode"><a class="viewcode-back" href="../../../../../api_reference/syft.core.tensor.autodp.gamma_tensor.html#syft.core.tensor.autodp.gamma_tensor.GammaTensor.decode">[docs]</a>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="n">FixedPrecisionTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">proxy_public_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;min_vals&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_vals</span><span class="p">,</span>
            <span class="s2">&quot;max_vals&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_vals</span><span class="p">,</span>
            <span class="s2">&quot;data_subjects&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_subjects</span><span class="p">,</span>
        <span class="p">}</span>  <span class="c1"># TODO 0.7: maybe this is obsolete now?</span>

    <span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">swap_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">GammaTensor</span><span class="p">(</span>
            <span class="n">child</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reconstruct</span><span class="p">(</span><span class="n">state</span><span class="p">),</span>
            <span class="n">sources</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
            <span class="n">jax_op</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">jax_op</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">astype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">GammaTensor</span><span class="p">(</span>
            <span class="n">child</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">new_type</span><span class="p">),</span>
            <span class="n">jax_op</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">jax_op</span><span class="p">,</span>
            <span class="n">sources</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sources</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;size&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">size</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> has no attribute size.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">jax_op</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="c1"># infix operations</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_infix_func</span><span class="p">(</span>
        <span class="n">left</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">right</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">gamma_op</span><span class="p">:</span> <span class="n">GAMMA_TENSOR_OP</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="n">left</span> <span class="o">=</span> <span class="n">debox_phi</span><span class="p">(</span><span class="n">left</span><span class="p">)</span>
        <span class="n">right</span> <span class="o">=</span> <span class="n">debox_phi</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">left</span><span class="o">.</span><span class="n">sources</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="s2">&quot;sources&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="n">output_state</span> <span class="o">=</span> <span class="n">update_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">GAMMA_TENSOR_OP_FUNC</span><span class="p">[</span><span class="n">gamma_op</span><span class="p">](</span><span class="n">debox_child</span><span class="p">(</span><span class="n">left</span><span class="p">),</span> <span class="n">debox_child</span><span class="p">(</span><span class="n">right</span><span class="p">))</span>
        <span class="n">is_linear</span> <span class="o">=</span> <span class="n">debox_linear</span><span class="p">(</span><span class="n">left</span><span class="p">)</span> <span class="ow">and</span> <span class="n">debox_linear</span><span class="p">(</span><span class="n">right</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_linear_op</span>
        <span class="n">jax_op</span> <span class="o">=</span> <span class="n">SyftJaxInfixOp</span><span class="p">(</span><span class="n">jax_op</span><span class="o">=</span><span class="n">gamma_op</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="n">right</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">GammaTensor</span><span class="p">(</span>
            <span class="n">child</span><span class="o">=</span><span class="n">child</span><span class="p">,</span> <span class="n">jax_op</span><span class="o">=</span><span class="n">jax_op</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="n">output_state</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="n">is_linear</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_rinfix</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">gamma_op</span><span class="p">:</span> <span class="n">GAMMA_TENSOR_OP</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix_func</span><span class="p">(</span>
            <span class="n">left</span><span class="o">=</span><span class="n">other</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">gamma_op</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="n">is_linear_op</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infix</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">gamma_op</span><span class="p">:</span> <span class="n">GAMMA_TENSOR_OP</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix_func</span><span class="p">(</span>
            <span class="n">left</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">gamma_op</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="n">is_linear_op</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_unary_op</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gamma_op</span><span class="p">:</span> <span class="n">GAMMA_TENSOR_OP</span><span class="p">,</span>
        <span class="n">is_linear</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">args</span> <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># can&#39;t use collections in default params</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="p">)</span>  <span class="c1"># can&#39;t use collections in default params</span>
        <span class="n">output_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sources</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">GAMMA_TENSOR_OP_FUNC</span><span class="p">[</span><span class="n">gamma_op</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">jax_op</span> <span class="o">=</span> <span class="n">SyftJaxUnaryOp</span><span class="p">(</span><span class="n">jax_op</span><span class="o">=</span><span class="n">gamma_op</span><span class="p">,</span> <span class="n">operand</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">GammaTensor</span><span class="p">(</span>
            <span class="n">child</span><span class="o">=</span><span class="n">child</span><span class="p">,</span>
            <span class="n">jax_op</span><span class="o">=</span><span class="n">jax_op</span><span class="p">,</span>
            <span class="n">sources</span><span class="o">=</span><span class="n">output_state</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="n">is_linear</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ADD</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SUBTRACT</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__mod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(other, gamma_op=GAMMA_TENSOR_OP.MOD, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MULTIPLY</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span>
            <span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">TRUE_DIVIDE</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span>
            <span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">FLOOR_DIVIDE</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MATMUL</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(other, gamma_op=GAMMA_TENSOR_OP.GREATER, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(</span>
        <span class="c1">#     other, gamma_op=GAMMA_TENSOR_OP.GREATER_EQUAL, is_linear_op=False</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(other, gamma_op=GAMMA_TENSOR_OP.LESS, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(</span>
        <span class="c1">#     other, gamma_op=GAMMA_TENSOR_OP.LESS_EQUAL, is_linear_op=False</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(other, gamma_op=GAMMA_TENSOR_OP.EQUAL, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(</span>
        <span class="c1">#     other, gamma_op=GAMMA_TENSOR_OP.NOT_EQUAL, is_linear_op=False</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="fm">__and__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(</span>
        <span class="c1">#     other, gamma_op=GAMMA_TENSOR_OP.BITWISE_AND, is_linear_op=False</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(</span>
        <span class="c1">#     other, gamma_op=GAMMA_TENSOR_OP.BITWISE_OR, is_linear_op=False</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="fm">__lshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(other, gamma_op=GAMMA_TENSOR_OP.LSHIFT, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__rshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(other, gamma_op=GAMMA_TENSOR_OP.RSHIFT, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__xor__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._infix(</span>
        <span class="c1">#     other, gamma_op=GAMMA_TENSOR_OP.BITWISE_XOR, is_linear_op=False</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="nf">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">GammaTensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="c1"># QUESTION: is there a reason other can&#39;t be a non gamma tensor numpy array?</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">DOT</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1">#  __r*__ infix operations</span>

    <span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="c1"># return self.__add__(other)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ADD</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="c1"># return (self.__sub__(other)) * -1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SUBTRACT</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._rinfix(other, gamma_op=GAMMA_TENSOR_OP.MOD, is_linear_op=False)</span>

    <span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MULTIPLY</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span>
            <span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">TRUE_DIVIDE</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span>
            <span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">FLOOR_DIVIDE</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MATMUL</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__divmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rinfix</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">DIVMOD</span><span class="p">,</span> <span class="n">is_linear_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">divmod</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__divmod__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="c1">#  unary operations</span>

    <span class="k">def</span> <span class="fm">__abs__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span><span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ABS</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._unary_op(</span>
        <span class="c1">#     gamma_op=GAMMA_TENSOR_OP.ARGMAX, is_linear=False, args=[axis]</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._unary_op(</span>
        <span class="c1">#     gamma_op=GAMMA_TENSOR_OP.ARGMIN, is_linear=False, args=[axis]</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>  <span class="c1"># TODO 0.7: this needs a test</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span><span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">LOG</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>  <span class="c1"># TODO 0.7: this needs a test</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">FLATTEN</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">order</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">TRANSPOSE</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">POWER</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ONES_LIKE</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ZEROS_LIKE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">filtered</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>  <span class="c1"># TODO</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return GammaTensor(</span>
        <span class="c1">#     child=jnp.zeros_like(self.child), jax_op=, sources=self.sources.copy()</span>
        <span class="c1"># )</span>

    <span class="c1"># def filtered(self) -&gt; GammaTensor:</span>
    <span class="c1">#     # This is only used during publish to filter out data in GammaTensors with no_op. It serves no other purpose.</span>
    <span class="c1">#     def _filtered(state: Dict) -&gt; GammaTensor:</span>
    <span class="c1">#         return self.reconstruct(state)</span>

    <span class="c1">#     func = _filtered</span>

    <span class="c1">#     return GammaTensor(</span>
    <span class="c1">#         child=jnp.zeros_like(self.child),</span>
    <span class="c1">#         func=func,</span>
    <span class="c1">#         sources=self.sources.copy(),</span>
    <span class="c1">#     )</span>

    <span class="k">def</span> <span class="fm">__round__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span><span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ROUND</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__round__</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SQUEEZE</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">ravel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">RAVEL</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">resize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">:</span> <span class="n">SingleOrTupleInt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">RESIZE</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">new_shape</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">compress</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">condition</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="n">output_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sources</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">child</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">jax_op</span> <span class="o">=</span> <span class="n">SyftJaxUnaryOp</span><span class="p">(</span>
            <span class="n">jax_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">COMPRESS</span><span class="p">,</span>
            <span class="n">operand</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">condition</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">},</span>
            <span class="n">operand_before</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">GammaTensor</span><span class="p">(</span>
            <span class="n">child</span><span class="o">=</span><span class="n">child</span><span class="p">,</span>
            <span class="n">jax_op</span><span class="o">=</span><span class="n">jax_op</span><span class="p">,</span>
            <span class="n">sources</span><span class="o">=</span><span class="n">output_state</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">any</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._unary_op(</span>
        <span class="c1">#     gamma_op=GAMMA_TENSOR_OP.ANY,</span>
        <span class="c1">#     is_linear=False,</span>
        <span class="c1">#     kwargs={&quot;axis&quot;: axis, &quot;keepdims&quot;: keepdims},</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="nf">all</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._unary_op(</span>
        <span class="c1">#     gamma_op=GAMMA_TENSOR_OP.ALL,</span>
        <span class="c1">#     is_linear=False,</span>
        <span class="c1">#     kwargs={&quot;axis&quot;: axis, &quot;keepdims&quot;: keepdims},</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="fm">__pos__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">POSITIVE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">NEGATIVE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">newshape</span><span class="p">:</span> <span class="n">SingleOrTupleInt</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">RESHAPE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">newshape</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;order&quot;</span><span class="p">:</span> <span class="n">order</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">std</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ddof</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">STD</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span>
                <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span>
                <span class="s2">&quot;ddof&quot;</span><span class="p">:</span> <span class="n">ddof</span><span class="p">,</span>
                <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">var</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ddof</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">VAR</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span>
                <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span>
                <span class="s2">&quot;ddof&quot;</span><span class="p">:</span> <span class="n">ddof</span><span class="p">,</span>
                <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SQRT</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">ABS</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">clip</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">a_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArrayLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">a_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArrayLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">CLIP</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">a_min</span><span class="p">,</span> <span class="n">a_max</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">nonzero</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._unary_op(gamma_op=GAMMA_TENSOR_OP.NONZERO, is_linear=False)</span>

    <span class="k">def</span> <span class="nf">swapaxes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">axis2</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SWAPAXES</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;__len__&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># nosec</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">,</span> <span class="n">ArrayLike</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="c1"># TODO: I think we can move the mapping of the final getattr(jnp, &quot;op&quot;) to be more general</span>
        <span class="c1"># to also accommodate this kind of pattern where its a lambda or whever</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">PY_GETITEM</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">,</span> <span class="n">ArrayLike</span><span class="p">],</span> <span class="n">value</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">GammaTensor</span><span class="p">,</span> <span class="n">ArrayLike</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># QUESTION: Is mutation allowed and if so how do we trace that in the graph?</span>
        <span class="c1"># self._unary_op(gamma_op=GAMMA_TENSOR_OP.PY_GETITEM, is_linear=True, args=[key])</span>
        <span class="c1"># # relative</span>
        <span class="c1"># from .phi_tensor import PhiTensor</span>

        <span class="c1"># # TODO: fix this</span>
        <span class="c1"># if isinstance(value, (PhiTensor, GammaTensor)):</span>
        <span class="c1">#     self.child[key] = value.child</span>
        <span class="c1"># elif isinstance(value, np.ndarray):</span>
        <span class="c1">#     self.child[key] = value</span>
        <span class="c1"># else:</span>
        <span class="c1">#     raise NotImplementedError</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">ptp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">OptionalAxisArg</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">PTP</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">take</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;clip&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">TAKE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="n">mode</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">put</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ind</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">v</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;raise&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">PUT</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="n">mode</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">repeats</span><span class="p">:</span> <span class="n">SingleOrTupleInt</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">REPEAT</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">repeats</span><span class="p">],</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">CUMSUM</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">CUMPROD</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lipschitz_bound</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">1.0</span>

        <span class="k">def</span> <span class="nf">convert_array_to_dict_state</span><span class="p">(</span><span class="n">array_state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">input_sizes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
            <span class="n">start_id</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">input_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">total_size</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
                    <span class="n">total_size</span> <span class="o">*=</span> <span class="n">size</span>
                <span class="n">state</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">array_state</span><span class="p">[</span><span class="n">start_id</span> <span class="p">:</span> <span class="n">start_id</span> <span class="o">+</span> <span class="n">total_size</span><span class="p">],</span> <span class="n">shape</span>  <span class="c1"># noqa: E203</span>
                <span class="p">)</span>
                <span class="n">start_id</span> <span class="o">+=</span> <span class="n">total_size</span>

            <span class="k">return</span> <span class="n">state</span>

        <span class="k">def</span> <span class="nf">convert_state_to_bounds</span><span class="p">(</span><span class="n">input_sizes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">input_states</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
            <span class="n">bounds</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">input_sizes</span><span class="p">:</span>
                <span class="n">bounds</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                    <span class="nb">list</span><span class="p">(</span>
                        <span class="nb">zip</span><span class="p">(</span>
                            <span class="n">input_states</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span><span class="o">.</span><span class="n">min_vals</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
                            <span class="n">input_states</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span><span class="o">.</span><span class="n">max_vals</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">bounds</span>

        <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">state</span><span class="p">))))</span>

        <span class="n">input_sizes</span> <span class="o">=</span> <span class="p">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">id</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sources</span><span class="o">.</span><span class="n">values</span><span class="p">()}</span>
        <span class="n">bounds</span> <span class="o">=</span> <span class="n">convert_state_to_bounds</span><span class="p">(</span><span class="n">input_sizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sources</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">array_state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jnp</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">:</span>
            <span class="n">dict_state</span> <span class="o">=</span> <span class="n">convert_array_to_dict_state</span><span class="p">(</span><span class="n">array_state</span><span class="p">,</span> <span class="n">input_sizes</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">dict_state</span><span class="p">)</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grads</span><span class="o">.</span><span class="n">values</span><span class="p">())))</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">shgo</span><span class="p">(</span><span class="n">search</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">sampling_method</span><span class="o">=</span><span class="s2">&quot;simplicial&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fun</span>

    <span class="k">def</span> <span class="nf">prod</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">PROD</span><span class="p">,</span> <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis1</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis2</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">TRACE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="n">offset</span><span class="p">,</span> <span class="s2">&quot;axis1&quot;</span><span class="p">:</span> <span class="n">axis1</span><span class="p">,</span> <span class="s2">&quot;axis2&quot;</span><span class="p">:</span> <span class="n">axis2</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">diagonal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis1</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis2</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">DIAGONAL</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="n">offset</span><span class="p">,</span> <span class="s2">&quot;axis1&quot;</span><span class="p">:</span> <span class="n">axis1</span><span class="p">,</span> <span class="s2">&quot;axis2&quot;</span><span class="p">:</span> <span class="n">axis2</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">min</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">initial</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MIN</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span> <span class="s2">&quot;initial&quot;</span><span class="p">:</span> <span class="n">initial</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">max</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">initial</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;keepdims&quot;</span><span class="p">:</span> <span class="n">keepdims</span><span class="p">,</span> <span class="s2">&quot;initial&quot;</span><span class="p">:</span> <span class="n">initial</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sort</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kind</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">SORT</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;kind&quot;</span><span class="p">:</span> <span class="n">kind</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">argsort</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kind</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="c1"># return self._unary_op(</span>
        <span class="c1">#     gamma_op=GAMMA_TENSOR_OP.ARGSORT,</span>
        <span class="c1">#     is_linear=False,</span>
        <span class="c1">#     kwargs={&quot;axis&quot;: axis, &quot;kind&quot;: kind},</span>
        <span class="c1"># )</span>

    <span class="k">def</span> <span class="nf">choose</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">choices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">PassthroughTensor</span><span class="p">],</span>
        <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;raise&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unary_op</span><span class="p">(</span>
            <span class="n">gamma_op</span><span class="o">=</span><span class="n">GAMMA_TENSOR_OP</span><span class="o">.</span><span class="n">CHOOSE</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;choices&quot;</span><span class="p">:</span> <span class="n">choices</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="n">mode</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="c1"># @staticmethod</span>
    <span class="c1"># def convert_dsl(</span>
    <span class="c1">#     state: dict, new_state: Optional[dict] = None</span>
    <span class="c1"># ) -&gt; Dict:  # TODO 0.7: maybe this is not required?</span>
    <span class="c1">#     if new_state is None:</span>
    <span class="c1">#         new_state = dict()</span>
    <span class="c1">#     if state:</span>
    <span class="c1">#         for tensor in list(state.values()):</span>
    <span class="c1">#             if isinstance(tensor.data_subjects, np.ndarray):</span>
    <span class="c1">#                 new_tensor = GammaTensor(</span>
    <span class="c1">#                     child=tensor.child,</span>
    <span class="c1">#                     func=tensor.func,</span>
    <span class="c1">#                     sources=GammaTensor.convert_dsl(tensor.sources, {}),</span>
    <span class="c1">#                 )</span>
    <span class="c1">#             else:</span>

    <span class="c1">#                 new_tensor = tensor</span>
    <span class="c1">#             new_state[new_tensor.id] = new_tensor</span>
    <span class="c1">#         return new_state</span>
    <span class="c1">#     else:</span>
    <span class="c1">#         return {}</span>

    <span class="k">def</span> <span class="nf">publish</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">get_budget_for_user</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">deduct_epsilon_for_user</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">ledger</span><span class="p">:</span> <span class="n">DataSubjectLedger</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">private</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">publish</span><span class="p">(</span>
            <span class="n">tensor</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">ledger</span><span class="o">=</span><span class="n">ledger</span><span class="p">,</span>
            <span class="n">get_budget_for_user</span><span class="o">=</span><span class="n">get_budget_for_user</span><span class="p">,</span>
            <span class="n">deduct_epsilon_for_user</span><span class="o">=</span><span class="n">deduct_epsilon_for_user</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">is_linear</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span><span class="p">,</span>
            <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">_object2bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bytes</span><span class="p">:</span>
        <span class="c1"># TODO Tudor: fix this</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">get_capnp_schema</span><span class="p">(</span><span class="n">schema_file</span><span class="o">=</span><span class="s2">&quot;gamma_tensor.capnp&quot;</span><span class="p">)</span>

        <span class="n">gamma_tensor_struct</span><span class="p">:</span> <span class="n">CapnpModule</span> <span class="o">=</span> <span class="n">schema</span><span class="o">.</span><span class="n">GammaTensor</span>  <span class="c1"># type: ignore</span>
        <span class="n">gamma_msg</span> <span class="o">=</span> <span class="n">gamma_tensor_struct</span><span class="o">.</span><span class="n">new_message</span><span class="p">()</span>
        <span class="c1"># this is how we dispatch correct deserialization of bytes</span>
        <span class="n">gamma_msg</span><span class="o">.</span><span class="n">magicHeader</span> <span class="o">=</span> <span class="n">serde_magic_header</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

        <span class="c1"># do we need to serde func? if so how?</span>
        <span class="c1"># what about the state dict?</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">):</span>
            <span class="n">chunk_bytes</span><span class="p">(</span><span class="n">capnp_serialize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">),</span> <span class="n">to_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&quot;child&quot;</span><span class="p">,</span> <span class="n">gamma_msg</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="n">gamma_msg</span><span class="o">.</span><span class="n">isNumpy</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">chunk_bytes</span><span class="p">(</span>
                <span class="n">capnp_serialize</span><span class="p">(</span><span class="n">jax2numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">to_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="s2">&quot;child&quot;</span><span class="p">,</span>
                <span class="n">gamma_msg</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">gamma_msg</span><span class="o">.</span><span class="n">isNumpy</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chunk_bytes</span><span class="p">(</span><span class="n">serialize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">,</span> <span class="n">to_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&quot;child&quot;</span><span class="p">,</span> <span class="n">gamma_msg</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="n">gamma_msg</span><span class="o">.</span><span class="n">isNumpy</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">gamma_msg</span><span class="o">.</span><span class="n">sources</span> <span class="o">=</span> <span class="n">serialize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sources</span><span class="p">,</span> <span class="n">to_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">gamma_msg</span><span class="o">.</span><span class="n">isLinear</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_linear</span>
        <span class="n">gamma_msg</span><span class="o">.</span><span class="n">id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span>
        <span class="n">gamma_msg</span><span class="o">.</span><span class="n">jaxOp</span> <span class="o">=</span> <span class="n">serialize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jax_op</span><span class="p">,</span> <span class="n">to_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># return gamma_msg.to_bytes_packed()</span>
        <span class="k">return</span> <span class="n">gamma_msg</span><span class="o">.</span><span class="n">to_bytes</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_bytes2object</span><span class="p">(</span><span class="n">buf</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GammaTensor</span><span class="p">:</span>
        <span class="c1"># TODO Tudor: fix this</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">get_capnp_schema</span><span class="p">(</span><span class="n">schema_file</span><span class="o">=</span><span class="s2">&quot;gamma_tensor.capnp&quot;</span><span class="p">)</span>
        <span class="n">gamma_struct</span><span class="p">:</span> <span class="n">CapnpModule</span> <span class="o">=</span> <span class="n">schema</span><span class="o">.</span><span class="n">GammaTensor</span>  <span class="c1"># type: ignore</span>
        <span class="c1"># https://stackoverflow.com/questions/48458839/capnproto-maximum-filesize</span>
        <span class="n">MAX_TRAVERSAL_LIMIT</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">64</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1"># capnp from_bytes is now a context</span>
        <span class="k">with</span> <span class="n">gamma_struct</span><span class="o">.</span><span class="n">from_bytes</span><span class="p">(</span>
            <span class="n">buf</span><span class="p">,</span> <span class="n">traversal_limit_in_words</span><span class="o">=</span><span class="n">MAX_TRAVERSAL_LIMIT</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">gamma_msg</span><span class="p">:</span>

            <span class="k">if</span> <span class="n">gamma_msg</span><span class="o">.</span><span class="n">isNumpy</span><span class="p">:</span>
                <span class="n">child</span> <span class="o">=</span> <span class="n">capnp_deserialize</span><span class="p">(</span>
                    <span class="n">combine_bytes</span><span class="p">(</span><span class="n">gamma_msg</span><span class="o">.</span><span class="n">child</span><span class="p">),</span> <span class="n">from_bytes</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">child</span> <span class="o">=</span> <span class="n">deserialize</span><span class="p">(</span><span class="n">combine_bytes</span><span class="p">(</span><span class="n">gamma_msg</span><span class="o">.</span><span class="n">child</span><span class="p">),</span> <span class="n">from_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">deserialize</span><span class="p">(</span><span class="n">gamma_msg</span><span class="o">.</span><span class="n">sources</span><span class="p">,</span> <span class="n">from_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">is_linear</span> <span class="o">=</span> <span class="n">gamma_msg</span><span class="o">.</span><span class="n">isLinear</span>
            <span class="n">id_str</span> <span class="o">=</span> <span class="n">UID</span><span class="o">.</span><span class="n">from_string</span><span class="p">(</span><span class="n">gamma_msg</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
            <span class="n">jax_op</span> <span class="o">=</span> <span class="n">deserialize</span><span class="p">(</span><span class="n">gamma_msg</span><span class="o">.</span><span class="n">jaxOp</span><span class="p">,</span> <span class="n">from_bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">GammaTensor</span><span class="p">(</span>
                <span class="n">child</span><span class="o">=</span><span class="n">child</span><span class="p">,</span>
                <span class="n">is_linear</span><span class="o">=</span><span class="n">is_linear</span><span class="p">,</span>
                <span class="n">sources</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
                <span class="nb">id</span><span class="o">=</span><span class="n">id_str</span><span class="p">,</span>
                <span class="n">jax_op</span><span class="o">=</span><span class="n">jax_op</span><span class="p">,</span>
            <span class="p">)</span></div>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Openmined Community.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.3.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>